[["index.html", "ML Workflow for Research Scientists Chapter 1 Preamble Who is this wiki for? What won’t you learn? What will you learn?", " ML Workflow for Research Scientists Ajinkya K Mulay 2021-02-08 Chapter 1 Preamble Who is this wiki for? There is a long list of Python and Github workflow tutorials. There are so many Machine Learning Tutorials too. However, there is no one easy to follow Machine Learning-ML Cloud Service-Local Development-Benchmarking Workflow. I am going to talk about three Free services which any person can use with basic internet access - Google Colaboratory (for ML Cloud Service), VS Code (Local ML Development) and Weights and Biases (for benchmarking). TL;DR- This is a process guide for deploying your ML models as a researcher! What won’t you learn? How to code in Python or develop Machine Leaning Models or learn Pytorch. What will you learn? How to contribute code to a well-structured projects Work with multiple Google Colaboratory Notebooks Create easy Python Documentation Scale up as your project and team grows in complexity and size Benchmark and Project Tracking "],["setup.html", "Chapter 2 ML Setup 2.1 Google Colaboratory (Colab) 2.2 Visual Studio (VS) Code (Local Option)", " Chapter 2 ML Setup 2.1 Google Colaboratory (Colab) The easiest way to get started with Machine Learning (for FREE!) is using Google Colaboratory. Let us look at the easiest out-of-the-box setup to get you started. Link your Google Account to store notebooks. Setting up storage. Adding files can easily be done by the Colab UI on the left pane of the notebook by clicking on the folder icon and clicking on the upload symbol. However, if your instance times out, which it easily can depending on your network, RAM usage, whether you are using Colab Pro or the Free version, you might prefer a solution which is more permanent. The permanent solution uses your Google Drive to host your files, especially larger datasets. There are ways to connect Colab to external data storage too like AWS. However, for most research purposes and smaller projects the Google Drive approach works well. You can check out other approaches at data storage connection. Let us now look at the Google Drive mounting method (reference) from google.colab import drive drive.mount(&#39;/content/drive&#39;) # Access the URL after running the command above and get authorized # To avoid slowdowns, transfer data from google drive to colab notebook zip_path = &#39;/content/drive/My Drive/Data/example-dataset.zip&#39; !cp &quot;{zip_path}&quot; . !unzip -q example-dataset.zip # Remove .zip file after you unzip it !rm split-garbage-dataset.zip # Make sure it&#39;s there !ls Here, we mount the drive using the first two commands. Next to speed training we transfer the actual data to the Colab notebook from drive. Here we have first stored our dataset as a zip file. We need to rerun these commands everytime we reconnect our Colab instance, but it can greatly speed up training and data management. Installing Python packages !pip3 install package-name Straightforward and most packages should be installed out-of-the-box. You can run other system commands by starting with the ! (exclamation mark) too. 2.2 Visual Studio (VS) Code (Local Option) Extensions If you have a really powerful machine, or if you just prefer running your files locally, I’d recommend using VS Code. Not only is it open-source, but it also has poweful extensions to emulate notebooks, code completion and a ton of python specific tools. My top recommended extensions are (in no specific order): Python for VSCode (for syntax) Visual Studio IntelliCode (for notebooks) Pylance (supercharge python) Code Runner (easily run code snippets) Installing Python packages You will need to have python3 and pip3 installed. pip3 install package-name "],["git.html", "Chapter 3 Git, Github and Desktop management 3.1 Using Git 3.2 Using GitHub and Google Colaboratory (Colab) 3.3 In-built option 3.4 The Longer Option", " Chapter 3 Git, Github and Desktop management The standard way to use Git is via the terminal. Another approach is to use Github’s official app at Github Desktop. It supports most common OS distributions. 3.1 Using Git Here are some standard methods for easy and error-free collaboration (for local files) NOTE: Adapted from R for Research Scientists 3.1.1 Setting up a local repository from a remote one Initialize First fork the upstream repository. Next access your remote fork’s url and clone it and set this remote url as the origin to push changes to. Finally, set the original repository (the one you cloned from) as your upstream. # clone parent repo git clone your-remote-fork-url # set forked repo as origin git set remote-url origin your-remote-fork-url # set parent repo as upstream git remote add upstream your-remote-parent-url # list all remote repo git remote -v Before starting to code Always, always and always fetch any changes from the upstream (parent) repository. Otherwise changes you make locally could create issues while merging (pushing your code upstream). # get all changes from the parent repo git fetch upstream # switch to master branch and merge all changes git checkout master git merge upstream/master Adding new features # you want your new feature to use the master as the base git checkout master # create a new branch to work on a feature git branch new-feature # switch to the new-feature branch git checkout new-feature Prevent tiny commits with amend Make sure your source code is also synced with a cloud service like dropbox or you are using Colab. This ensures you do not lose data due to not saving or due to power failures. # make changes to code # commit git commit -m &#39;your message&#39; # make few small changes # do not commit another message, just amend git commit --amend # accumulate until you have a large enough commit Incorporate upstream changes before rebasing feature branch # get all changes from the parent repo git fetch upstream # switch to master branch and merge all changes git checkout master git merge upstream/master # switch to new-feature branch git checkout new-feature # new-feature commits switched to top of the master branch git pull --rebase origin master If you get stuck use the following link to troubleshoot further, troubleshoot or google to check it out on stackoverflow 3.2 Using GitHub and Google Colaboratory (Colab) 3.3 In-built option Create a repository on GitHub or decide to use an existing one Go to you Colab file Next, select File-&gt;Save a copy in GitHub Authorize your GitHub Account and select the appropriate repository, add the commit message and press OK! To use version control, make changes to your code and repeat steps (2)-(4) 3.4 The Longer Option Colab provides the option to download your source code to your computer. You can go to File-&gt;Download .ipynb OR File-&gt;Download .py and store the notebook as a local notebook or convert to a python file. Next, you can use the local Git way to maintain version control. To access the file in Colab, you’ll however have to reupload the file to Colab using the File-&gt;Open Notebook option in Colab. "],["testing.html", "Chapter 4 Unit Testing", " Chapter 4 Unit Testing The simplest way to create unit tests in python are using the module unittest. Let us look at a simple example (from GeeksforGeeks) # Python code to demonstrate working of unittest import unittest class TestStringMethods(unittest.TestCase): # Returns True if the string contains 4 a. def test_strings_a(self): self.assertEqual( &#39;a&#39;*4, &#39;aaaa&#39;) # Returns True if the string is in upper case. def test_upper(self): self.assertEqual(&#39;foo&#39;.upper(), &#39;FOO&#39;) # Run all unit tests if __name__ == &#39;__main__&#39;: unittest.main() The above example is pretty self-explanatory. It tests two simple functions- multiplying a string by 4 times and converting a string to uppercase. These tests are super useful for deterministic functions. However, creating such tests for gradient descent might not be easy. However, if we know the possible bounds on the function’s output we can create functions to test them. A simple ML testing library is indeed available for tensorflow (deprecated in TF2.0) and pytorch. unittest could well be used to define something similar as demonstrated in pytorch code review. These libraries check for the following items: * Whether variables change * If certain variables are fixed, do they remain fixed over iterations * Is the output value reasonable (i.e. under certain bounds)? * Is the model well connected (i.e. do all inputs contriibute to the training operation)? "],["doc.html", "Chapter 5 Documentation", " Chapter 5 Documentation Maintaning documentation is hard. The easiest way to get to automated documentation (not recommended, have a look at How I Judge Documentation Quality?) is using pdoc. Let’s get started with installation and maintenance. Install pdoc for python-3 using the following command pip3 install pdoc3 To automate documentation creation we need to add comments using the Google docstrings’ format. Let us look at a quick example for a function used to add two numbers (stored in the file Test_Example.py) def add_binary(a, b): &#39;&#39;&#39; Returns the sum of two decimal numbers in binary digits. Parameters: a (int): A decimal integer b (int): Another decimal integer Returns: binary_sum (str): Binary string of the sum of a and b &#39;&#39;&#39; binary_sum = bin(a+b)[2:] return binary_sum Now, to output the documentation in html files, we will use the following command. pdoc --html Test_Example.py --output-dir docs Here, the Test_Example.py is the source code. The output-dir parameter provides the location to store the html files for the wiki. The documentation looks like the following. However, this command will not work for a .ipynb file. The best way to create documentation will be to download the .py file from Colab and then use the pdoc command. Another way to convert a notebook to a python file is to use the nbconvert command. ipython nbconvert --to python notebook.ipynb Furthermore, to create documentation for the entire python source code folder, we can use the following pdoc commmand. Now, to output the documentation in html files, we will use the following command. pdoc --html src-folder --output-dir docs Here, src-folder is the folder holding Python source-code. "],["benchmarking.html", "Chapter 6 Benchmarking", " Chapter 6 Benchmarking ML Systems get super complex very easily. You’ve to track projects, experiments within these projects, metrics inside each experiment, hyperparameters for each metric collection, and so much more. Furthermore, multiple experiment runs slow the experimentation phase and makes it harder to track the results. Recently, the ML systems got an excellent upgrade. Now, users could pipe out all the super interesting stuff onto a cool looking dashboard. Enter Weights and Biases. You not only get a free account as an academic, but you can also invite a team to collaborate! Especially if you work on research and/or in open-source. Weights and Biases (WANDB) exists to give you control of your data, store multiple experimentation runs and compare your results easily! Let us look at the simplest way to get WANDB running on your project. You can find a similar code setup once you create your project on &lt;wandb.ai&gt;. Init pip install --upgrade wandb wandb login your-login-code Sample Training Loop to log metrics # Init wandb import wandb # List of hyperparameters config = dict ( learning_rate = 0.01, momentum = 0.2, architecture = &quot;CNN&quot;, dataset_id = &quot;peds-0192&quot;, infra = &quot;AWS&quot;, ) # Initialize the project with the hyperparameters and other # organization stuff wandb.init(project=&quot;project-name&quot;, notes=&quot;your-notes&quot;, tags=[&quot;baselie&quot;, &quot;tag1&quot;], config=config, entity=&#39;teamname&#39; ) # Save your dataset to track which dataset was used in the current # expt artifact = wandb.Artifact(&#39;my-dataset&#39;, type=&#39;dataset&#39;) artifact.add_file(&#39;my-dataset.txt&#39;) # Log metrics (here accuracy) with wandb wandb.watch(model) # Here, we assume that the my-dataset.txt has been used to create a # train_loader model.train() for batch_idx, (data, target) in enumerate(train_loader) if batch_idx % args.log_interval == 0: wandb.log({&quot;Test Accuracy&quot;: correct / total, &quot;Test Loss&quot;: loss}) # Save model to wandb torch.save(model.state_dict(), os.path.join(wandb.run.dir, &#39;model.pt&#39;)) "],["putting-it-all-together.html", "Chapter 7 Putting It All Together 7.1 Workflow 7.2 Concluding Remarks", " Chapter 7 Putting It All Together 7.1 Workflow How to start with ML Projects! An overview of the entire workflow to good collaboration on ML projects. 7.2 Concluding Remarks Getting started with Colab and VS Code is super easy. However, to avoid hassles for long-term projects &amp; collaborations it is best to establish sane practices early on. Hopefully, following this guide and the many references on here give you the neccessary peace of mind. Code on! "],["references.html", "References", " References N. Brewer, “Publications in RStudio,” Mar. 10, 2020. https://nicole-brewer.com/r-for-research-scientists/ (accessed Feb. 06, 2021). "]]
